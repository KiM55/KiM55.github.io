

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head><TITLE>TransZero++: Cross Attribute-Guided Transformer for Zero-Shot Learning</TITLE>
<META content="text/html; charset=UTF-8" http-equiv=Content-Type>

<META content=IE=Edge,chrome=1 http-equiv=X-UA-Compatible>
<LINK rel=stylesheet href="style.css" media=screen>
<STYLE type=text/css>
@media Print
{
#STTBimg {
    DISPLAY: none
}
#STTBimg2 {
    DISPLAY: none
}
    }
</STYLE>
<style type="text/css">
      body{background:#FFFFFF;text-align:center;}
      div{width:970px;margin:0 auto;background:#fff;text-align:left;}

      
.STYLE2 {font-size: 18px}
.STYLE6 {font-size: 24pt}
.STYLE8 {color: #CC0000}
.STYLE14 {color: #000000; font-size: 17px; font-weight: bold; }
.STYLE15 {color: #000000}
.STYLE16 {font-size: 22px}
.STYLE20 {color: #FF0000; font-weight: bold; }
.STYLE22 {color: #0000FF}
</style>


<META name=GENERATOR content="MSHTML 9.00.8112.16441"></head>
<body class=" hasGoogleVoiceExt" screen_capture_injected="true">

<div class="container"> 
      <br><br>
	  <h1 class="STYLE6" style="TEXT-ALIGN: center">TransZero++: Cross Attribute-Guided Transformer for Zero-Shot Learning</H1>
	  <br>
	  <span class="STYLE2"><br>
      </span>
     <p style='text-align: center;'> 
       <span style="font-size: 18px"><a class='urllink' href='https://shiming-chen.github.io/' title='' rel='nofollow'><font color="#0000FF"><strong>Shiming Chen</strong></font></a><sup>1</sup>,
        <a class='urllink' href='' title='' rel='nofollow'><font color="#0000FF"><strong>Ziming Hong</strong></font></a><sup>1</sup>,
	<a class='urllink' href='' title='' rel='nofollow'><font color="#0000FF"><strong>Wenjin Hou</strong></font></a><sup>1</sup>,
        <a class='urllink' href='https://scholar.google.com/citations?user=LKaWa9gAAAAJ' title='' rel='nofollow'><font color="#0000FF"><strong>Guo-Sen Xie</strong></font></a><sup>2</sup>,
	<a class='urllink' href='https://ybsong00.github.io/' title='' rel='nofollow'><font color="#0000FF"><strong>Yibing Song</strong></font></a><sup>3</sup>,
	<a class='urllink' href='https://zhaoj9014.github.io/' title='' rel='nofollow'><font color="#0000FF"><strong>Jian Zhao</strong></font></a><sup>4</sup>,
	<a class='urllink' href='http://bmal.hust.edu.cn/info/1005/1091.htm' title='' rel='nofollow'><font color="#0000FF"><strong>Xinge You</strong></font></a><sup>1</sup>,
	</span></p>
        <p style='text-align: center;'> 
        <a class='urllink' href='https://yanshuicheng.ai/' title='' rel='nofollow'><font color="#0000FF"><strong>Shuicheng Yan</strong></font></a><sup>5</sup>,
	<a class='urllink' href='https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en' title='' rel='nofollow'><font color="#0000FF"><strong>Ling Shao</strong></font></a><sup>6</sup>
      </span><br>     
     </p>
	
      <p class='vspace STYLE2' style='text-align: center;'>
        <sup>1</sup>Huazhong University of Science and Technology (HUST), China &nbsp &nbsp <sup>2</sup>Nanjing University of Science and Technology, China</p>
      <p class='vspace STYLE2' style='text-align: center;'>
        <sup>3</sup> Tencent AI Lab, Shenzhen, China   &nbsp &nbsp<sup>4</sup>Institute of North Electronic Equipment, China</p>
	  <p class='vspace STYLE2' style='text-align: center;'>
        <sup>5</sup>Sea AI Lab (SAIL), Singapore &nbsp &nbsp &nbsp <sup>6</sup> Terminus Group </p>
      <p class='vspace STYLE2' style='text-align: center;'>
          <font color="#0000FF" font-style='italic' font-family='NimbusMonL-Regu'>{shimingchen, youxg}@hust.edu.cn  &nbsp &nbsp {hoongzm, gsxiehm}@gmail.com  &nbsp &nbsp  zhaojian90@u.nus.edu </br> &nbsp&nbsp shuicheng.yan@gmail.com &nbsp &nbsp ling.shao@ieee.org </font></p>

      <div class=vspace> </div>
	  <div class=line> </div>
	  <br>
	  
      <h2 align="center">Abstract</h2>
      <P align="justify"> Zero-shot learning (ZSL) tackles the novel class recognition problem by transferring semantic knowledge from seen classes to unseen ones. 
	      Semantic knowledge is typically represented by attribute descriptions shared between different classes, {\color{blue}which act as strong priors for
	      localizing object attributes that represent discriminative region features,} enabling significant and sufficient  visual-semantic interaction for 
	      advancing ZSL. Existing attention-based models have struggled to learn inferior region features in a single image by solely using unidirectional 
	      attention, {\color{blue}which ignore the transferable and discriminative attribute localization of visual features for representing the key semantic
	      knowledge for effective knowledge transfer in ZSL.} In this paper, we propose a cross attribute-guided Transformer network, termed TransZero++,
	      to refine visual features and learn accurate attribute localization for key semantic knowledge representations in ZSL. Specifically, 
	      TransZero++ employs an attribute$\rightarrow$visual Transformer sub-net (AVT) and a visual$\rightarrow$attribute Transformer sub-net (VAT) to learn
	      attribute-based visual features and visual-based attribute features, respectively. By further introducing feature-level and prediction-level semantical
	      collaborative losses, the two attribute-guided transformers teach each other to learn semantic-augmented visual embeddings for key semantic knowledge representations via semantical collaborative
	      learning. Finally, the semantic-augmented visual embeddings learned by AVT and VAT are fused to conduct desirable visual-semantic interaction cooperated
	      with class semantic vectors for ZSL classification. Extensive experiments show that TransZero++ achieves the new state-of-the-art results on three
	      golden and challenging ZSL benchmarks. The project website is at: \url{https://shiming-chen.github.io/TransZero-pp/TransZero-pp.html}.</P>
	  <div class=line> </div>
	  <p></p>
	  
	  
	  <h2>Model pipeline </h2>
      
	  <div class="div-table-col-12" align='center'>
                <center><img src="figures/TransZero-pipeline.png" width="750" height="400"></center>
				<p></p>
				<P align="justify"><strong>Figure:</strong>The architecture of the proposed TransZero++ model. TransZero++ consists of an attribute$\rightarrow$visual Transformer sub-net (AVT) and a visual$\rightarrow$attribute Transformer sub-net (VAT). AVT includes a feature augmentation encoder that alleviates the cross-dataset bias between ImageNet and ZSL benchmarks and reduces the entangled geometry relationships between different regions for improving the transferability of visual features, and an attribute$\rightarrow$visual decoder that localizes object attributes for attribute-based visual feature representations based on the semantic attribute information. Analogously, VAT learns visual-based attribute features using the similar feature augmentation encoder and a visual$\rightarrow$attribute decoder.  Finally, two mapping functions $\mathcal{M}_1$ and $\mathcal{M}_2$ map the learned attribute-based visual features and visual-based attribute features into semantic embedding space respectively under the guidance of semantical collaborative learning, enabling desirable visual-semantic interaction for ZSL classification.</P>
				
				
	  </div>
	 <div class=line> </div>
	  <p></p>
	  
	  
       <h2>Material </h2>
      
	  
	 <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="div-table-row">
			  	<div class="div-table-col-3" align='center' id="img-large" >
                <center><a class=urllink title="" rel=nofollow><img src="figures/PAMI.png" width="150" height="150" class="layered-paper-big"></br></br></br><h3><font color="#0000FF"><strong>Paper</strong></font></h3></a></center>
				</div>
				<div class="div-table-col-3" align='center' id="img-large1" style="margin-left:150px">
                <!--<center><a class=urllink title="" href="https://github.com/shiming-chen/Similariy-DT" rel=nofollow><img src="Videos-gif/github-figure.jpg" width="150" height="150"></br></br></br><h3><font color="#0000FF"><strong>Code</strong><br></font>&nbsp<font color="#FF0000"><strong>(Coming soon)</strong></font></h3></a></center>-->
				<center><a class=urllink title="" href="https://github.com/shiming-chen/TransZero_pp" rel=nofollow><img src="figures/github-figure.jpg" width="150" height="150"></br></br></br><h3><font color="#0000FF"><strong>Code</strong></font></h3></a></center>
	                        </div>
	  </div>	
	  </div>  
	   </form>
      
          <div class=vspace></div>
	
	  <!-- <div> -->
	   
	    <!-- <p>If you wish to use our code, please cite the following paper : </p> -->

	  
	   <!-- <div class=div-table-reference> -->
        <!-- <strong>Similarity-DT: Kernel Similarity Embedding for Dynamic Texture Synthesis</strong><br> -->
        <!-- Shiming Chen, Peng Zhang, Xinge You, Qinmu Peng, Xin Liu, Zehong Cao, Dacheng Tao<br> -->
        <!-- <em>arXiv preprint arXiv: 1911.04254, </em> 2019<br>		 -->
      <!-- </div>	    -->
    <!-- </div> -->

      <div class=vspace></div>     
	   
	   <div class=line> </div>
	  <p></p>
	  <h2>Evaluation</h2>
	  <p class="STYLE15">There are three popular challenging benchmark datasets used for evaluating our method: <a class=urllink title="" href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html" rel=nofollow><font size="4.2" font-weight="bold" color="#0000FF">CUB</font></a>, <a class=urllink title="" href="http://cs.brown.edu/~gmpatter/sunattributes.html" rel=nofollow><font size="4.2" color="#0000FF">SUN</font></a>, and <a class=urllink title="" href="http://cvml.ist.ac.at/AwA2/" rel=nofollow><font size="4.2" color="#0000FF">AWA2</font></a>. Some sampls are presented below.
      </p>
	  <cneter><p>
		   <span class="STYLE15">We conduct experiments both in <strong>Conventional Zero-Shot Learning (CZSL)</strong> and <strong>Generalized Zero-Shot Learning (GZSL)</strong> settings. In the CZSL setting, we predict the unseen classes to compute the accuracy of test samples, i.e., <strong>ACC</strong>. In the GZSL setting,  we calculate the accuracy of the test samples from both the seen classes (denoted as <strong>S</strong>) and unseen classes (denoted as <strong>U</strong>), and their harmonic mean <strong>H</strong>.
	   </span></p></center>
	  <div class="div-table-col-12" align='center'>
                <center><img src="figures/samples.png" width="920" height="300"></center></br>
	  </div>
	  
	  <div class=line> </div>
	  
	  <h3 class="STYLE16" id="exp1">Experiment 1: Comparison with State-of-the-Art</h3>
	  <div class="div-table-col-12" align='center'>
                <center><img src="figures/Exp1.png" width="920" height="650"></center>
				<p></p>
	  </div>
	 <div class=line> </div>
	  <p></p>
	  
	  <h3 class="STYLE16">Experiment 2: Ablation Study </h3>
	  <div class="div-table-col-12" align='center'>
                <center><img src="figures/Exp2-A1.png" width="920" height="250"></center>
				</br>
				<center><img src="figures/Exp2-A2.png" width="920" height="280"></center>
				</br>
				<center><img src="figures/Exp2-A3.png" width="920" height="200"></center>
				<p></p>
	  </div>
	 <div class=line> </div>
	  <p></p>
	  
	  <h3 class="STYLE16">Experiment 3: t-SNE Visualization</h3>
	  <p class="STYLE15">t-SNE visualizations of visual features for <strong>(a) CUB</strong>, <strong>(b) SUN</strong> and <strong>(c) AWA2</strong>,
			learned by the CNN backbone, TransZero++(AVT) encoder w/o FA, TransZero++(AVT) encoder, TransZero++(AVT) decoder, TransZero++(VAT) encoder w/o FA,
			TransZero++(VAT) encoder, TransZero++(VAT) decoder and TransZero++(VAT and VAT). The 10 colors denote 10 different seen/unseen classes randomly selected
			from these datasets. Results show that our various model components in TransZero++ learn the discriminative visual feature representations,
			while CNN backbone (e.g., ResNet101) failed. <strong>For each group, the first two rows are the visualizations of visual feature on seen classes, and the other two rows are on unseen classes. </strong></p>
	  <div class="div-table-col-12" align='center'>
                <center><img src="figures/CUB-Seen.png" width="920" height="480"></center>
				<center><img src="figures/CUB-Unseen.png" width="920" height="480"></center>
				<center><h3>(a) CUB</h3></center>	
				</br>
				<center><img src="figures/SUN-Seen.png" width="920" height="480"></center>
				<center><img src="figures/SUN-Unseen.png" width="920" height="480"></center>
				<center><h3>(b) SUN</h3></center>	
				</br>
				<center><img src="figures/AWA2-Seen.png" width="920" height="480"></center>
				<center><img src="figures/AWA2-Unseen.png" width="920" height="480"></center>
				<center><h3>(c) AWA2</h3></center>	
				<p></p>
	  </div>
	 <div class=line> </div>
	  <p></p>
	  
	  <h3 class="STYLE16">Experiment 3: Attention Maps</h3>
	  <p class="STYLE15">Visualization of top-10 attention maps for the <strong>AREN [2](top)</strong>, <strong>TransZero [3] (middle)</strong> and <strong>our TransZero++ (bottom) </strong>in each group. Results show that TransZero localizes some important object attributes with low confident scores for representing region features, while AREN is failed. Furthermore, our TransZero++ discovers more valuable attributes that exist in the corresponding image with high confident scores compared to TransZero.</p>
	  <div class="div-table-col-12" align='center'>
                <center><img src="figures/AREN/Acadian_Flycatcher_0008_795599.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Acadian_Flycatcher_0008_795599.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Acadian_Flycatcher_0008_795599.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/American_Goldfinch_0092_32910.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/American_Goldfinch_0092_32910.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/American_Goldfinch_0092_32910.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Canada_Warbler_0117_162394.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Canada_Warbler_0117_162394.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Canada_Warbler_0117_162394.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Carolina_Wren_0006_186742.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Carolina_Wren_0006_186742.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Carolina_Wren_0006_186742.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Elegant_Tern_0085_151091.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Elegant_Tern_0085_151091.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Elegant_Tern_0085_151091.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/European_Goldfinch_0025_794647.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/European_Goldfinch_0025_794647.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/European_Goldfinch_0025_794647.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Florida_Jay_0008_64482.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Florida_Jay_0008_64482.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Florida_Jay_0008_64482.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Fox_Sparrow_0025_114555.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Fox_Sparrow_0025_114555.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Fox_Sparrow_0025_114555.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Grasshopper_Sparrow_0053_115991.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Grasshopper_Sparrow_0053_115991.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Grasshopper_Sparrow_0053_115991.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Grasshopper_Sparrow_0107_116286.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Grasshopper_Sparrow_0107_116286.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Grasshopper_Sparrow_0107_116286.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Gray_Crowned_Rosy_Finch_0036_797287.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Gray_Crowned_Rosy_Finch_0036_797287.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Gray_Crowned_Rosy_Finch_0036_797287.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Red_Legged_Kittiwake_0062_795434.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Red_Legged_Kittiwake_0062_795434.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Red_Legged_Kittiwake_0062_795434.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Vesper_Sparrow_0090_125690.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Vesper_Sparrow_0090_125690.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Vesper_Sparrow_0090_125690.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Western_Gull_0058_53882.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Western_Gull_0058_53882.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Western_Gull_0058_53882.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/White_Throated_Sparrow_0128_128956.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/White_Throated_Sparrow_0128_128956.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/White_Throated_Sparrow_0128_128956.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Winter_Wren_0118_189805.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Winter_Wren_0118_189805.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Winter_Wren_0118_189805.jpg" width="920" height="120"></center>
				</br>
				<center><img src="figures/AREN/Yellow_Breasted_Chat_0044_22106.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ/Yellow_Breasted_Chat_0044_22106.jpg" width="920" height="120"></center>
				<center><img src="figures/TZ-PP/Yellow_Breasted_Chat_0044_22106.jpg" width="920" height="120"></center>
				<p></p>
	  </div>
	  <p></p>
	  
	  
	  
	   </form>
	   
	
	  
	  
	 
	   

	 
	    <!--<center><img src="../../images/animation_hummingbird.gif" width=50% alt="" align="middle"></center> -->

	  <div class=line> </div>
      <!-- <div class=line-seg> </div> -->
      <p></p>
      <h2>Acknowledgement</h2>

      <p align="justify">This work is partially supported by NSFC~(61772220,62006244), Special projects for technological innovation in Hubei Province~(2018ACA135),
	      Key R\&D Plan of Hubei Province~(2020BAB027) and 2020-2022 Young Elite Scientist Sponsorship Program from China Association for Science and Technology
	      YESS20200140.</p>
      <div class=line> </div>
      
	   <p></p>
	   <h2>Reference</h2>
	  <p align="justify">[1] Yongqin Xian et al. "Zero-Shot Learning: A Comprehensive Evaluation of the Good, the Bad and the Ugly." <em>In TPAMI</em>, 2019.</p>
      <p align="justify">[2] Guo-Sen Xie et al. "Attentive region embedding network for zero-shot learning." <em>In CVPR)</em>, 2019.</p>
	  <p align="justify">[3] Shiming Chen  et al. "Transzero: Attribute-guided transformer for zero-shot learning." <em>In AAAI</em>, 2022.</p>
      <p align="justify">[4] Wenjia Xu  et al. "Attribute Prototype Network for Zero-Shot Learning." <em>In NeurIPS</em>, 2020.</p>
      
      <div class=line> </div>
      <h4> <center><a class='urllink' href='./STGConvNet.html' title='' rel='nofollow'><font color="#0000FF">Top</font></a> </center></h4>
</div>
</body></html>
